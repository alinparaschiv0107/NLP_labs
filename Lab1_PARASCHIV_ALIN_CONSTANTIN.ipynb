{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1 - PARASCHIV ALIN-CONSTANTIN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Lab overview:\n",
        "\n",
        "\n",
        "* Normalization\n",
        "* Tokenization\n",
        "* Lematization\n",
        "* Stemming\n",
        "* Stopwords removal\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nK8jcjSNkv3"
      },
      "source": [
        "##Text normalization (cleaning)\n",
        "\n",
        "Depending on the task you are cleaning the text for, you may perform one or more of: \n",
        "\n",
        "* Transform text to lowercase\n",
        "* Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "* Remove punctuation\n",
        "* Remove digits or transform them to words\n",
        "* Correct spelling errors\n",
        "\n",
        "\n",
        "Python Regular Expressions \n",
        "*   [re Python documentation](https://docs.python.org/3/library/re.html)\n",
        "*   [Quick reference](https://www.computerhope.com/unix/regex-quickref.htm)\n",
        "*   [Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)\n",
        "\n",
        "![regular_expressions](https://res.cloudinary.com/practicaldev/image/fetch/s--_iE0KvdT--/c_imagga_scale,f_auto,fl_progressive,h_900,q_auto,w_1600/https://dev-to-uploads.s3.amazonaws.com/i/zpek00ubevoxvn458b01.png)\n",
        "\n",
        "[Photo source](https://dev.to/mconner89/regular-expressions-grouping-and-string-methods-3ijn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzKaNrWZ16E"
      },
      "source": [
        "Here is our text sample, a short review of the movie [Jaws](https://en.wikipedia.org/wiki/Jaws_(film))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwbmESJGWAre",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f606cb5-ad5c-49c7-ea0e-8b0f138d1de3"
      },
      "source": [
        "text = '\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'\n",
        "text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-zwIaLJ_gI"
      },
      "source": [
        "Transform text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEhAwxLyaX7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32d6a3e3-f086-45be-cb9a-b65ffb503d28"
      },
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htuAG8zKarGo"
      },
      "source": [
        "importing [re](https://docs.python.org/3/library/re.html) library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOeMPOhga09l"
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ioMoqMP4sW"
      },
      "source": [
        "Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PM0guG_P60M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bbefaeb4-b779-4f7a-a57d-8928c7e94727"
      },
      "source": [
        "re.sub(' \\d+', '', text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves stars, not stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQEFmcfRPYc"
      },
      "source": [
        "Converting numbers to words using [num2words](https://github.com/savoirfairelinux/num2words) (it works on multiple languages)\n",
        "\n",
        "We need to install the num2words library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx3KhgnRPfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ce9c20-bae6-466b-ede9-3808bbdae1d9"
      },
      "source": [
        "!pip install num2words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.7/dist-packages (0.5.10)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1K45N29RlOX"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Egr4m_9Rl9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2df1bcaf-3bce-443a-d842-62a0f751757f"
      },
      "source": [
        "from num2words import num2words\n",
        "\n",
        "text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
        "text\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEeAr6pKFj0"
      },
      "source": [
        "Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "\n",
        "Using [emoji](https://github.com/carpedm20/emoji) library or the corresponding unicode characters.\n",
        "\n",
        "We need to install the emoji library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq72wdvEKI7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf5fca7-4cf5-46e5-c40d-f6f691673dab"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVu11QZEKoSW"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HYmbrutKnEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44d04704-d3c9-4963-c85a-2d031961e339"
      },
      "source": [
        "import emoji\n",
        "\n",
        "emoji.get_emoji_regexp().sub(u'', text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69dNffKLtqu"
      },
      "source": [
        "The *get_emoji_regexp()* function returns a regex to match any emoji.\n",
        "\n",
        "Another way of removing emojis with regex:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSh8NUIM_bY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a947344e-f4fb-4bef-d224-f0ec25e98091"
      },
      "source": [
        "emoj = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2640-\\u2642\" \n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"\n",
        "    u\"\\u3030\"\n",
        "    \"]+\", re.UNICODE)\n",
        "\n",
        "text = re.sub(emoj, '', text)\n",
        "text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_xacsPCOPjL"
      },
      "source": [
        "Removing emoticons (regex from [nltk Twitter Tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYkCRPNOS3n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16074cc7-c2a2-4f9d-eacf-3065c01d5147"
      },
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      </?3                       # heart\n",
        "    )\"\"\"\n",
        "    \n",
        "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "text = re.sub(emoticon_re, '', text)\n",
        "text"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds.   it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VkbsLqXMyYo"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "*   Word level: Split by whitespace, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "*   Sentence level: Split by punctuation, [nltk.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrN047ULHZQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2536689-d448-4f1c-a87a-1ea17cadc60b"
      },
      "source": [
        "print(text.split())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', 'jaws', '\"', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen.', 'the', 'movie', 'opens', 'with', 'blackness,', 'and', 'only', 'distant,', 'alien-like', 'underwater', 'sounds.', 'it', 'deserves', 'five', 'stars,', 'not', 'four', 'stars.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyTVP55VRwT"
      },
      "source": [
        "We need to download first the Punkt Tokenizer Models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHvjfBzxVdAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386dcf79-3e9e-4ba9-ca9f-96a6afa4f1d3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nZasrqFVE-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31267348-a3ec-4fd8-e1db-a86a9510b7d2"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "tokenized_text_nltk = word_tokenize(text)\n",
        "print(tokenized_text_nltk)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'jaws', '``', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen', '.', 'the', 'movie', 'opens', 'with', 'blackness', ',', 'and', 'only', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'it', 'deserves', 'five', 'stars', ',', 'not', 'four', 'stars', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yypjBlMVpnG"
      },
      "source": [
        "Sentence tokenization using regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqqArIwwVp1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c45f093-bcfc-417a-ba84-47f46c11eef3"
      },
      "source": [
        " re.split('(?<=[.!?]) +', text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7KKoyA1WNVo"
      },
      "source": [
        "Sentence tokenization using nltk.sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3RfpgBzWSaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90175d51-b665-4083-f19b-996d16059181"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9idFgSWnJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44601b18-0f95-44fd-9bf0-5d14ef3f3328"
      },
      "source": [
        "text_example = 'I was good.Thanks.'\n",
        "re.split('(?<=[.!?]) +', text_example)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0DKB0VoWr8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66580afe-d937-4bf9-873a-5354ae608398"
      },
      "source": [
        "nltk.sent_tokenize(text_example)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s4GXfHXCOl"
      },
      "source": [
        "Removing punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCxHclC0XF8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8168fcb-37e1-4deb-f715-2414b53460e1"
      },
      "source": [
        "re.sub(r'[^\\w\\s]','', text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws   is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYxNlaSgi6gH"
      },
      "source": [
        "Using [string](https://docs.python.org/3/library/string.html) library. \n",
        "\n",
        "The string.punctuation method returns a list of punctuation marks. \n",
        "\n",
        "We use the translate() method which replaces every instance of a punctuation mark with the value '' in our strings. We use the str.maketrans() method to support the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jid14LagYAPQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63c02de4-e26f-447c-a760-f1276c69a0ac"
      },
      "source": [
        "import string\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws   is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkRYWmakLla"
      },
      "source": [
        "Removing multiple spaces between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAFnlSzYu7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b932435-31b8-4ba7-a48c-f9c8503ee353"
      },
      "source": [
        "text = re.sub(' +', ' ', text)\n",
        "text"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F0pEMhIHbd6"
      },
      "source": [
        "## Removing stopwords\n",
        "\n",
        "![stopwords.jpg](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png) \n",
        "\n",
        "[Photo source](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980726-remove-stop-words-from-a-block-of-text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53_oLK7NP8y"
      },
      "source": [
        "###Why do we Need to Remove Stopwords?\n",
        "\n",
        "For tasks such as text classification, we may want to remove any unnecessary words and keep only words with meaning. \n",
        "\n",
        "Stopwords removal is not used in tasks such as machine translation or text summarization.\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGkYhh5oMcx1"
      },
      "source": [
        "Stopwords removal using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idPo-mzrC2HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa6c14c-7511-4541-f043-42229dfe2797"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "179\n",
            "{'that', 'will', 'who', 'between', 'm', 'own', 'being', 'now', 'off', 'him', 'before', 'very', \"isn't\", 'her', 'but', \"don't\", 'because', 'mightn', 'have', 'them', 'be', 'again', 'your', 'my', 'am', 're', 'most', 'wasn', \"it's\", \"you'll\", 'than', 'during', 'to', 'the', 'should', 'as', 'then', 'down', 'himself', 'having', 'are', 'he', 'needn', 'our', \"needn't\", \"mustn't\", 'no', 'were', 'can', 'other', 'this', 'aren', 'out', 'until', 'while', 'doing', \"didn't\", 'i', 'o', 'through', 'there', 'and', \"you'd\", 'at', \"mightn't\", 'didn', 'why', 's', 'll', 'yourselves', \"doesn't\", 'shan', 'when', 'did', 'a', 'all', 'such', 'ain', 'haven', \"weren't\", 'herself', 'up', 'these', 'with', 'we', \"shouldn't\", 'you', 'under', 'yourself', 'been', 'ours', 'after', 'ourselves', 'they', 'itself', 'just', 'me', 'their', 'both', 'so', \"couldn't\", \"should've\", 've', \"that'll\", 'on', 'y', 'by', 'some', 'few', 'from', \"she's\", 'she', 'which', 'themselves', 'only', \"won't\", 'theirs', 'too', 'those', 'yours', 'into', 'against', 'over', \"haven't\", 'does', 'below', 'what', 'here', 'once', 'same', 'don', 'had', \"wasn't\", 'myself', 't', 'not', 'if', 'of', 'in', 'further', 'about', 'has', 'whom', 'won', 'doesn', 'hers', 'for', 'how', 'ma', \"you're\", 'mustn', \"hasn't\", 'shouldn', \"aren't\", 'where', 'hasn', \"hadn't\", 'his', 'an', 'or', 'any', 'wouldn', 'is', 'nor', 'was', 'couldn', \"shan't\", 'its', 'hadn', \"wouldn't\", 'above', 'd', 'more', 'isn', \"you've\", 'weren', 'do', 'each', 'it'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY7q0410L7t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97987b8-6e87-415f-81d0-2e2bcca1b77d"
      },
      "source": [
        "tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'jaws', '``', 'rare', 'film', 'grabs', 'attention', 'shows', 'single', 'image', 'screen', '.', 'movie', 'opens', 'blackness', ',', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'deserves', 'five', 'stars', ',', 'four', 'stars', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3FIzVTMYc7"
      },
      "source": [
        "Stopwords removal using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys1CTPNoMgNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "64869e7f-3ab1-47b2-bc54-8bb9e74fbbc5"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:766: UserWarning: [W094] Model 'en_core_web_sm' (2.2.5) specifies an under-constrained spaCy version requirement: >=2.2.2. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.1.3,<3.2.0\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d8a10013f083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstop_words_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_spacy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_spacy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     return util.load_model(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \"\"\"\n\u001b[1;32m    379\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         return config.from_disk(\n\u001b[1;32m    573\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config.cfg from /usr/local/lib/python3.7/dist-packages/en_core_web_sm/en_core_web_sm-2.2.5/config.cfg"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE9rct__Oi7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1bd185b6-be1f-4b9e-cee7-fb49a66e8a0e"
      },
      "source": [
        "tokenized_text_spacy = nlp(text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0bdae1a64156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_text_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_text_without_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_spacy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words_spacy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_without_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH15qHZ3HYga"
      },
      "source": [
        "## Lematization/Stemming\n",
        "\n",
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)\n",
        "\n",
        "[Photo source](https://tr.pinterest.com/pin/706854104005417976/)\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOVeBOOYZo7F"
      },
      "source": [
        "Lematization\n",
        "\n",
        "Using the WordNetLemmatizer from nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89_jM6NY_nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3230c675-50c5-4f7b-fbc7-d0360ca7714b"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxu3jUM1UviR"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = word_tokenize(text)\n",
        "for word in words:\n",
        "    print(word, lemmatizer.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWPz08R0qD79"
      },
      "source": [
        "Using the [lemmatizer](https://spacy.io/api/lemmatizer) from spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5oA4G1muu4y"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YnUMJpqEO3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMUayDqZweHR"
      },
      "source": [
        "Stemming in using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN7dDNx1weU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65fe4ab6-3a7b-479c-9d25-3e658c66fc0d"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word, ps.stem(word))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaws jaw\n",
            "is is\n",
            "a a\n",
            "rare rare\n",
            "film film\n",
            "that that\n",
            "grabs grab\n",
            "your your\n",
            "attention attent\n",
            "before befor\n",
            "it it\n",
            "shows show\n",
            "you you\n",
            "a a\n",
            "single singl\n",
            "image imag\n",
            "on on\n",
            "screen screen\n",
            "the the\n",
            "movie movi\n",
            "opens open\n",
            "with with\n",
            "blackness black\n",
            "and and\n",
            "only onli\n",
            "distant distant\n",
            "alienlike alienlik\n",
            "underwater underwat\n",
            "sounds sound\n",
            "it it\n",
            "deserves deserv\n",
            "five five\n",
            "stars star\n",
            "not not\n",
            "four four\n",
            "stars star\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmgsTVhIZuS9"
      },
      "source": [
        "[Other stemmers in nltk](https://www.nltk.org/api/nltk.stem.html)\n",
        "\n",
        "The spacy library does not perform stemming, only lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/ygCNwFM4i5RMPtsC6\n",
        "\n",
        "Preprocess texts from Twitter\n",
        "\n",
        "## Data\n",
        "\n",
        "We will use the twitter corpus from nltk, usually used in sentiment analysis.\n",
        "\n",
        "The fist step is downloading the dataset using the *download* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fLAUv1MRdHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fd3621-1f95-407b-bc61-15d27e3c928e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_VdfEmjKXh"
      },
      "source": [
        "In order to inspect our data, we look at the first 25 tweets from the dataset. The text contains a lot of mentions, hashtags and emoticons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIODYYSvSuWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df10ed6f-9c68-4cfc-e894-677d767fb1bc"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "tweets = tweets[:25]\n",
        "tweets"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
              " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
              " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
              " '@Impatientraider On second thought, there‚Äôs just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
              " 'Jgh , but we have to go to Bayan :D bye',\n",
              " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell‚Ä¶ as the name implies :p.',\n",
              " '#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0',\n",
              " 'Laying out a greetings card range for print today - love my job :-)',\n",
              " \"Friend's lunch... yummmm :)\\n#Nostalgia #TBS #KU.\",\n",
              " \"@RookieSenpai @arcadester it is the id conflict thanks for the help :D here's the screenshot of it working\",\n",
              " '@oohdawg_ Hi liv :))',\n",
              " 'Hello I need to know something can u fm me on Twitter?? ‚Äî sure thing :) dm me x http://t.co/W6Dy130BV7',\n",
              " '#FollowFriday @MBandScott_ @Eric_FLE @pointsolutions3 for being top new followers in my community this week :)',\n",
              " \"@rossbreadmore I've heard the Four Seasons is pretty dope. Penthouse, obvs #Gobigorgohome\\nHave fun y'all :)\",\n",
              " '@gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))',\n",
              " 'Hello :) Get Youth Job Opportunities follow &gt;&gt; @tolajobjobs @maphisa301',\n",
              " \"üíÖüèΩüíã - :)))) haven't seen you in years\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V_Y5pHtc07i"
      },
      "source": [
        "**Given a list of tweets, preprocess each tweet from the list.**\n",
        "\n",
        "**Instructions**: Implement the *preprocess* function. You can do the text cleaning in any order you prefer.\n",
        "\n",
        "**Hint**: You may need to use regex expressions (use the resources provided above).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAmrMgS7QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be9b2ab-5f25-443f-f545-f6a9dfb64fd9"
      },
      "source": [
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess(tweets):\n",
        "\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        tweets: a list of tweets\n",
        "    Output: \n",
        "        prepocessed_tweets: a list of preprocessed tweets\n",
        "    \"\"\"\n",
        "\n",
        "    ###you may need to create an additional list in which to store the processed tweets\n",
        "    ###pay attention that some of the cleaning steps can be done at the document level, while others may be computed at word level\n",
        "\n",
        "    prepocessed_tweets = []\n",
        "    stop_words_nltk = set(stopwords.words('english'))\n",
        "    for tweet in tweets:\n",
        "\n",
        "        ###remove new line characters '\\n'\n",
        "        tweet = re.sub('\\n', '', tweet)\n",
        "        \n",
        "        ###remove links http://t.co/of3DyOzML0\n",
        "        tweet = re.sub('http\\S+', '', tweet)\n",
        "        \n",
        "        ###remove mentions '@'\n",
        "        tweet = re.sub('@\\S+', '', tweet)\n",
        "        \n",
        "        ###remove hashtags '#'\n",
        "        tweet = re.sub('#', '', tweet)\n",
        "        \n",
        "        ###lowercase text\n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        ###remove emojis and emoticons 'üëå üç≠ :) :D'\n",
        "        tweet = emoji.get_emoji_regexp().sub(u'', tweet)\n",
        "        tweet = re.sub(emoticon_re, '', tweet)\n",
        "        \n",
        "        ###remove digits\n",
        "        tweet = re.sub('\\d+', '', tweet)\n",
        "        \n",
        "        ###remove punctuation\n",
        "        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "        \n",
        "        ###tokenize tweet into separate words\n",
        "        ###remove stopwords\n",
        "        tweet = ' '.join([i for i in word_tokenize(tweet) if not i in stop_words_nltk])\n",
        "        \n",
        "        ###lematization or stemming\n",
        "        tweet = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(tweet)])\n",
        "        #tweet = ' '.join([ps.stem(word) for word in word_tokenize(tweet)])\n",
        "\n",
        "        prepocessed_tweets.append(tweet)\n",
        "    \n",
        "    return prepocessed_tweets\n",
        "\n",
        "preprocess(tweets)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday top engaged member community week',\n",
              " 'hey james odd please call contact centre able assist many thanks',\n",
              " 'listen last night bleed amazing track scotland',\n",
              " 'congrats',\n",
              " 'yeaaaah yippppy accnt verified rqst succeed got blue tick mark fb profile day',\n",
              " 'one irresistible flipkartfashionfriday',\n",
              " 'dont like keep lovely customer waiting long hope enjoy happy friday lwwf',\n",
              " 'second thought there enough time dd new short entering system sheep must buying',\n",
              " 'jgh go bayan bye',\n",
              " 'act mischievousness calling etl layer inhouse warehousing app katamariwell name implies',\n",
              " 'followfriday top influencers community week',\n",
              " 'wouldnt love bigjuicyselfies',\n",
              " 'follow follow u back',\n",
              " 'perfect already know whats waiting',\n",
              " 'great new opportunity junior triathletes aged gatorade series get entry',\n",
              " 'laying greeting card range print today love job',\n",
              " 'friend lunch yummmm nostalgia tb ku',\n",
              " 'id conflict thanks help here screenshot working',\n",
              " 'hi liv',\n",
              " 'hello need know something u fm twitter sure thing dm x',\n",
              " 'followfriday top new follower community week',\n",
              " 'ive heard four season pretty dope penthouse obvs gobigorgohomehave fun yall',\n",
              " 'yeah suppose lol chat bit x',\n",
              " 'hello get youth job opportunity follow gtgt',\n",
              " 'havent seen year']"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Tools:\n",
        "\n",
        "* [Preprocessing library for Twitter](https://github.com/s/preprocessor)\n",
        "* [Emoji library](https://github.com/carpedm20/emoji)\n",
        "* [Demoji library](https://github.com/bsolomon1124/demoji)\n",
        "* [Gensim](https://radimrehurek.com/gensim/)\n",
        "\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [Lexical Normalization](https://arxiv.org/pdf/1710.03476.pdf)\n",
        "* [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://aclanthology.org/2021.naacl-main.50.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}