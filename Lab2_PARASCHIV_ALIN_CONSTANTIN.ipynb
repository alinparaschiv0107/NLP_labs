{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2 - PARASCHIV ALIN-CONSTANTIN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing for non-English languages\n",
        "\n",
        "##Challenges\n",
        "* Diacritics restoration \n",
        "* Text segmentation for Chinese, Japanese, Arabic\n",
        "* You may need to have some knowledge about the language\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eee1lFEWQgq9"
      },
      "source": [
        "##Case studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu6CzKPSQof-"
      },
      "source": [
        "###Romanian\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nWQUPlZaKR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1507e1-9e92-408b-e43c-7400bf7f5776"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "romanian_text = '''Examenul Chunin începe 25. A zecea întrebare: Totul sau nimic!\n",
        "    Excepţie a făcut-o Hotinul, unde s-a aşezat o garnizoană turcească, care a început să jefuiască cu cruzime ţara.\n",
        "    Excitarea catatonică este o stare de agitaţie constantă şi de excitare motrică şi nervoasă.\n",
        "    Excreţiile parazitului irită pielea prpducând mâncărime şi răni produse de scărpinat, iau naştere papule, vezicule, pustule, infiltraţii, acestea prin infecţii secundare, se pot transforma în furunculi.\n",
        "    Executarea tabloului este precedată de o întreagă serie de schiţe.\n",
        "    Execuţia acestei lucrări a început în anul 1957 şi s-a finalizat în anul urmator.\n",
        "    Exemple de mesaje au fost eliminarea unui al doilea membru al tribului după ce a fost eliminat unul sau evitarea participării la Consiliul Tribal, dar cu preţul mutării intr-o locaţie mai puţin confortabilă.\n",
        "    Exemple de specii din grupul calmarilor pot servi Loligo vulgaris, Ommastrephes sagittatus, Ommastrephes slosnei pacificus, Chiroteuthis veranyi etc. Unele specii sunt exploatate pentru carnea lor comestibilă\n",
        "    '''\n",
        "pprint(romanian_text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Examenul Chunin începe 25. A zecea întrebare: Totul sau nimic!\\n'\n",
            " '    Excepţie a făcut-o Hotinul, unde s-a aşezat o garnizoană turcească, care '\n",
            " 'a început să jefuiască cu cruzime ţara.\\n'\n",
            " '    Excitarea catatonică este o stare de agitaţie constantă şi de excitare '\n",
            " 'motrică şi nervoasă.\\n'\n",
            " '    Excreţiile parazitului irită pielea prpducând mâncărime şi răni produse '\n",
            " 'de scărpinat, iau naştere papule, vezicule, pustule, infiltraţii, acestea '\n",
            " 'prin infecţii secundare, se pot transforma în furunculi.\\n'\n",
            " '    Executarea tabloului este precedată de o întreagă serie de schiţe.\\n'\n",
            " '    Execuţia acestei lucrări a început în anul 1957 şi s-a finalizat în anul '\n",
            " 'urmator.\\n'\n",
            " '    Exemple de mesaje au fost eliminarea unui al doilea membru al tribului '\n",
            " 'după ce a fost eliminat unul sau evitarea participării la Consiliul Tribal, '\n",
            " 'dar cu preţul mutării intr-o locaţie mai puţin confortabilă.\\n'\n",
            " '    Exemple de specii din grupul calmarilor pot servi Loligo vulgaris, '\n",
            " 'Ommastrephes sagittatus, Ommastrephes slosnei pacificus, Chiroteuthis '\n",
            " 'veranyi etc. Unele specii sunt exploatate pentru carnea lor comestibilă\\n'\n",
            " '    ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SwK7ZIa3Gt"
      },
      "source": [
        "Lowercasing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIrLujmWa5Mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97305b85-16b1-4bfb-c1d9-f41561df5a58"
      },
      "source": [
        "romanian_text = romanian_text.lower()\n",
        "pprint(romanian_text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('examenul chunin începe 25. a zecea întrebare: totul sau nimic!\\n'\n",
            " '    excepţie a făcut-o hotinul, unde s-a aşezat o garnizoană turcească, care '\n",
            " 'a început să jefuiască cu cruzime ţara.\\n'\n",
            " '    excitarea catatonică este o stare de agitaţie constantă şi de excitare '\n",
            " 'motrică şi nervoasă.\\n'\n",
            " '    excreţiile parazitului irită pielea prpducând mâncărime şi răni produse '\n",
            " 'de scărpinat, iau naştere papule, vezicule, pustule, infiltraţii, acestea '\n",
            " 'prin infecţii secundare, se pot transforma în furunculi.\\n'\n",
            " '    executarea tabloului este precedată de o întreagă serie de schiţe.\\n'\n",
            " '    execuţia acestei lucrări a început în anul 1957 şi s-a finalizat în anul '\n",
            " 'urmator.\\n'\n",
            " '    exemple de mesaje au fost eliminarea unui al doilea membru al tribului '\n",
            " 'după ce a fost eliminat unul sau evitarea participării la consiliul tribal, '\n",
            " 'dar cu preţul mutării intr-o locaţie mai puţin confortabilă.\\n'\n",
            " '    exemple de specii din grupul calmarilor pot servi loligo vulgaris, '\n",
            " 'ommastrephes sagittatus, ommastrephes slosnei pacificus, chiroteuthis '\n",
            " 'veranyi etc. unele specii sunt exploatate pentru carnea lor comestibilă\\n'\n",
            " '    ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBJNRbnzbAIe"
      },
      "source": [
        "Removing digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmFfE5VSbCHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4adbc4fd-214a-439f-dea7-8df62536e1b6"
      },
      "source": [
        "import re\n",
        "\n",
        "romanian_text = re.sub(' \\d+', '', romanian_text)\n",
        "pprint(romanian_text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('examenul chunin începe. a zecea întrebare: totul sau nimic!\\n'\n",
            " '    excepţie a făcut-o hotinul, unde s-a aşezat o garnizoană turcească, care '\n",
            " 'a început să jefuiască cu cruzime ţara.\\n'\n",
            " '    excitarea catatonică este o stare de agitaţie constantă şi de excitare '\n",
            " 'motrică şi nervoasă.\\n'\n",
            " '    excreţiile parazitului irită pielea prpducând mâncărime şi răni produse '\n",
            " 'de scărpinat, iau naştere papule, vezicule, pustule, infiltraţii, acestea '\n",
            " 'prin infecţii secundare, se pot transforma în furunculi.\\n'\n",
            " '    executarea tabloului este precedată de o întreagă serie de schiţe.\\n'\n",
            " '    execuţia acestei lucrări a început în anul şi s-a finalizat în anul '\n",
            " 'urmator.\\n'\n",
            " '    exemple de mesaje au fost eliminarea unui al doilea membru al tribului '\n",
            " 'după ce a fost eliminat unul sau evitarea participării la consiliul tribal, '\n",
            " 'dar cu preţul mutării intr-o locaţie mai puţin confortabilă.\\n'\n",
            " '    exemple de specii din grupul calmarilor pot servi loligo vulgaris, '\n",
            " 'ommastrephes sagittatus, ommastrephes slosnei pacificus, chiroteuthis '\n",
            " 'veranyi etc. unele specii sunt exploatate pentru carnea lor comestibilă\\n'\n",
            " '    ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3vte5EFb7x0"
      },
      "source": [
        "Removing diacritics using [Unidecode](https://pypi.org/project/Unidecode/). Transforming Unicode text in ASCII."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxU-1bXcGBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df87d3ab-9ddf-4602-8f71-2e5c77ce6ec4"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2LOnLH5b9nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46858abe-8977-4900-8768-bb25a71dc4c8"
      },
      "source": [
        "import unidecode\n",
        "romanian_text  = unidecode.unidecode(romanian_text)\n",
        "pprint(romanian_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('examenul chunin incepe. a zecea intrebare: totul sau nimic!\\n'\n",
            " '    exceptie a facut-o hotinul, unde s-a asezat o garnizoana turceasca, care '\n",
            " 'a inceput sa jefuiasca cu cruzime tara.\\n'\n",
            " '    excitarea catatonica este o stare de agitatie constanta si de excitare '\n",
            " 'motrica si nervoasa.\\n'\n",
            " '    excretiile parazitului irita pielea prpducand mancarime si rani produse '\n",
            " 'de scarpinat, iau nastere papule, vezicule, pustule, infiltratii, acestea '\n",
            " 'prin infectii secundare, se pot transforma in furunculi.\\n'\n",
            " '    executarea tabloului este precedata de o intreaga serie de schite.\\n'\n",
            " '    executia acestei lucrari a inceput in anul si s-a finalizat in anul '\n",
            " 'urmator.\\n'\n",
            " '    exemple de mesaje au fost eliminarea unui al doilea membru al tribului '\n",
            " 'dupa ce a fost eliminat unul sau evitarea participarii la consiliul tribal, '\n",
            " 'dar cu pretul mutarii intr-o locatie mai putin confortabila.\\n'\n",
            " '    exemple de specii din grupul calmarilor pot servi loligo vulgaris, '\n",
            " 'ommastrephes sagittatus, ommastrephes slosnei pacificus, chiroteuthis '\n",
            " 'veranyi etc. unele specii sunt exploatate pentru carnea lor comestibila\\n'\n",
            " '    ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu_mud4-bMoA"
      },
      "source": [
        "Sentence tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9KEQgmbRey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b381b2-cf09-45d6-af6b-a42ba600612b"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sent_romanian = sent_tokenize(romanian_text)\n",
        "sent_romanian "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['examenul chunin incepe.',\n",
              " 'a zecea intrebare: totul sau nimic!',\n",
              " 'exceptie a facut-o hotinul, unde s-a asezat o garnizoana turceasca, care a inceput sa jefuiasca cu cruzime tara.',\n",
              " 'excitarea catatonica este o stare de agitatie constanta si de excitare motrica si nervoasa.',\n",
              " 'excretiile parazitului irita pielea prpducand mancarime si rani produse de scarpinat, iau nastere papule, vezicule, pustule, infiltratii, acestea prin infectii secundare, se pot transforma in furunculi.',\n",
              " 'executarea tabloului este precedata de o intreaga serie de schite.',\n",
              " 'executia acestei lucrari a inceput in anul si s-a finalizat in anul urmator.',\n",
              " 'exemple de mesaje au fost eliminarea unui al doilea membru al tribului dupa ce a fost eliminat unul sau evitarea participarii la consiliul tribal, dar cu pretul mutarii intr-o locatie mai putin confortabila.',\n",
              " 'exemple de specii din grupul calmarilor pot servi loligo vulgaris, ommastrephes sagittatus, ommastrephes slosnei pacificus, chiroteuthis veranyi etc.',\n",
              " 'unele specii sunt exploatate pentru carnea lor comestibila']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAhei-_DbuMB"
      },
      "source": [
        "Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZSvPmlVdYZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4067d5-56de-42c7-b13c-1280838ee1bd"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import string\n",
        "\n",
        "\n",
        "sent_romanian = [sent.translate(str.maketrans('', '', string.punctuation)) for sent in sent_romanian]\n",
        "\n",
        "words_romanian = [[word_tokenize(sent) for sent in sent_romanian]]\n",
        "pprint(words_romanian) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['examenul', 'chunin', 'incepe'],\n",
            "  ['a', 'zecea', 'intrebare', 'totul', 'sau', 'nimic'],\n",
            "  ['exceptie',\n",
            "   'a',\n",
            "   'facuto',\n",
            "   'hotinul',\n",
            "   'unde',\n",
            "   'sa',\n",
            "   'asezat',\n",
            "   'o',\n",
            "   'garnizoana',\n",
            "   'turceasca',\n",
            "   'care',\n",
            "   'a',\n",
            "   'inceput',\n",
            "   'sa',\n",
            "   'jefuiasca',\n",
            "   'cu',\n",
            "   'cruzime',\n",
            "   'tara'],\n",
            "  ['excitarea',\n",
            "   'catatonica',\n",
            "   'este',\n",
            "   'o',\n",
            "   'stare',\n",
            "   'de',\n",
            "   'agitatie',\n",
            "   'constanta',\n",
            "   'si',\n",
            "   'de',\n",
            "   'excitare',\n",
            "   'motrica',\n",
            "   'si',\n",
            "   'nervoasa'],\n",
            "  ['excretiile',\n",
            "   'parazitului',\n",
            "   'irita',\n",
            "   'pielea',\n",
            "   'prpducand',\n",
            "   'mancarime',\n",
            "   'si',\n",
            "   'rani',\n",
            "   'produse',\n",
            "   'de',\n",
            "   'scarpinat',\n",
            "   'iau',\n",
            "   'nastere',\n",
            "   'papule',\n",
            "   'vezicule',\n",
            "   'pustule',\n",
            "   'infiltratii',\n",
            "   'acestea',\n",
            "   'prin',\n",
            "   'infectii',\n",
            "   'secundare',\n",
            "   'se',\n",
            "   'pot',\n",
            "   'transforma',\n",
            "   'in',\n",
            "   'furunculi'],\n",
            "  ['executarea',\n",
            "   'tabloului',\n",
            "   'este',\n",
            "   'precedata',\n",
            "   'de',\n",
            "   'o',\n",
            "   'intreaga',\n",
            "   'serie',\n",
            "   'de',\n",
            "   'schite'],\n",
            "  ['executia',\n",
            "   'acestei',\n",
            "   'lucrari',\n",
            "   'a',\n",
            "   'inceput',\n",
            "   'in',\n",
            "   'anul',\n",
            "   'si',\n",
            "   'sa',\n",
            "   'finalizat',\n",
            "   'in',\n",
            "   'anul',\n",
            "   'urmator'],\n",
            "  ['exemple',\n",
            "   'de',\n",
            "   'mesaje',\n",
            "   'au',\n",
            "   'fost',\n",
            "   'eliminarea',\n",
            "   'unui',\n",
            "   'al',\n",
            "   'doilea',\n",
            "   'membru',\n",
            "   'al',\n",
            "   'tribului',\n",
            "   'dupa',\n",
            "   'ce',\n",
            "   'a',\n",
            "   'fost',\n",
            "   'eliminat',\n",
            "   'unul',\n",
            "   'sau',\n",
            "   'evitarea',\n",
            "   'participarii',\n",
            "   'la',\n",
            "   'consiliul',\n",
            "   'tribal',\n",
            "   'dar',\n",
            "   'cu',\n",
            "   'pretul',\n",
            "   'mutarii',\n",
            "   'intro',\n",
            "   'locatie',\n",
            "   'mai',\n",
            "   'putin',\n",
            "   'confortabila'],\n",
            "  ['exemple',\n",
            "   'de',\n",
            "   'specii',\n",
            "   'din',\n",
            "   'grupul',\n",
            "   'calmarilor',\n",
            "   'pot',\n",
            "   'servi',\n",
            "   'loligo',\n",
            "   'vulgaris',\n",
            "   'ommastrephes',\n",
            "   'sagittatus',\n",
            "   'ommastrephes',\n",
            "   'slosnei',\n",
            "   'pacificus',\n",
            "   'chiroteuthis',\n",
            "   'veranyi',\n",
            "   'etc'],\n",
            "  ['unele',\n",
            "   'specii',\n",
            "   'sunt',\n",
            "   'exploatate',\n",
            "   'pentru',\n",
            "   'carnea',\n",
            "   'lor',\n",
            "   'comestibila']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14iGgZq-7cW"
      },
      "source": [
        "spaCy on Romanian text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjn9U8a5Qqxm"
      },
      "source": [
        "###Japanese \n",
        "\n",
        "[Japanese punctuation](https://en.wikipedia.org/wiki/Japanese_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajsK1qxrGu4S"
      },
      "source": [
        "japanese_text = '''$100ベットしていた場合には$100の配当を得られる。\n",
        "    なお、100形のうち101 - 110の10両は当時丸ノ内線方南町支線用であり 、これの代替には1500N形投入により2000形を10両 (2031 - 2040) 捻出して対応している。\n",
        "    コンセプトは「100時間遊べるおまけ」。\n",
        "    なお100系1･2次車の床面高さは従前通りの1150mmである。\n",
        "    ゴチ10・12・15ではMCを担当。\n",
        "    ! 101番目の魔物 （ 大海恵 ） 2005年 * 劇場版 金色のガッシュベル!\n",
        "    1088年に誕生した人物及び著名な動物 。'''"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMzZlhIcHaDM"
      },
      "source": [
        "Removing digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At5P_Fd-Hb_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8a235577-5742-4032-b9e8-91384cb9e627"
      },
      "source": [
        "import re\n",
        "\n",
        "japanese_text = ''.join([c for c in japanese_text if c.isdigit() == False ])\n",
        "japanese_text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'$ベットしていた場合には$の配当を得られる。\\n    なお、形のうち - の両は当時丸ノ内線方南町支線用であり 、これの代替にはN形投入により形を両 ( - ) 捻出して対応している。\\n    コンセプトは「時間遊べるおまけ」。\\n    なお系･次車の床面高さは従前通りのmmである。\\n    ゴチ・・ではMCを担当。\\n    ! 番目の魔物 （ 大海恵 ） 年 * 劇場版 金色のガッシュベル!\\n    年に誕生した人物及び著名な動物 。'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zid78ObFMwS_"
      },
      "source": [
        "Installing some dependencies for japanese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBN4bFN0gB48"
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXhJJWhNgS-b"
      },
      "source": [
        "!pip install fugashi[unidic-lite]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua13p15FEPIR"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ja_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0mRJHgKGefO"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apvAwCqTEh94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac1d2c1-e0b2-46d8-e0f2-03541edf876c"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154\n",
            "{'たり', 'のち', 'いつ', 'て', 'いずれ', 'す', 'でき', 'いっ', 'らしい', 'ここ', 'とも', 'ながら', 'や', 'なく', 'あれ', 'よう', 'な', 'ね', 'なお', 'もと', 'よっ', 'また', '一', 'あ', 'さらに', 'しよう', 'そこ', 'が', 'いう', 'か', 'たら', 'なかっ', 'こう', 'おり', 'ほか', 'すぐ', 'る', 'たい', 'に', 'あまり', 'こと', 'くん', 'のみ', 'ご', 'おけ', 'いく', 'だ', 'まま', 'しか', 'よれ', 'かつ', 'れ', 'れる', 'せい', 'い', 'よ', 'あり', 'すべて', 'いい', 'べき', 'それ', 'ば', 'そう', 'する', 'は', 'た', 'よる', 'し', 'よく', 'さ', 'かつて', 'しかし', 'うち', 'だけ', 'おい', 'られる', 'あるいは', 'ほぼ', 'を', 'など', 'にて', 'まで', 'ほど', 'き', 'ごと', 'ら', 'で', 'み', 'どう', 'の', 'しまう', 'これ', 'とき', 'へ', 'なし', 'なる', 'ぬ', 'つ', 'せ', 'たち', 'こ', 'つけ', 'も', 'もっ', 'ち', 'より', 'やっ', 'いる', 'あっ', 'なら', 'できる', 'つつ', 'なっ', 'はじめ', 'せる', 'とっ', 'ぶり', 'かなり', 'もの', 'ほとんど', 'ます', 'その', 'そして', 'ため', 'それぞれ', 'られ', 'ちゃん', 'きっかけ', 'ず', 'ひと', 'おら', 'なり', 'ただし', 'および', 'なけれ', 'ある', 'いわ', 'と', 'から', 'くる', 'ん', 'ない', 'お', 'だっ', 'ま', 'です', 'さん', 'え', 'この', 'ところ', 'つい', 'しまっ', 'もう', 'かけ'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwrehuGrGbqx"
      },
      "source": [
        "tokenized_text_spacy = nlp(japanese_text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywO8_YjJXXr"
      },
      "source": [
        "Removing punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g4dBNDwgMHy"
      },
      "source": [
        "import spacy\n",
        "for word in tokenized_text_without_stopwords:\n",
        "    if word.is_punct:\n",
        "        print(word, word.lemma_, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO2krQx8JZZe"
      },
      "source": [
        "for word in tokenized_text_without_stopwords:\n",
        "    if word.is_punct == False:\n",
        "        print(word, word.lemma_, word.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/wLsjCWnxK7w8GPvt9\n",
        "\n",
        "Choose samples from 2 languages and preprocess the texts (normalization, tokenization, lematization, etc.).\n",
        "\n",
        "Try to use spacy or find other resources and tools for the chosen languages.\n",
        "\n",
        "Also mention if the language you have choosen needs specific preprocessing.\n",
        "\n",
        "**Please add the resources you used in the doc**: https://docs.google.com/document/d/1c5sqPfgSioGzLZkRv4yWw7DWiiQC96QHw34ZRqcDiSY/edit?usp=sharing for further reference.\n",
        "\n",
        "\n",
        "Questions: If you have chosen a language you are fluent in, how well did the tools work on this language? What problems did you observed (e.g. problems with tokenization, etc.)\n",
        "\n",
        "## Data\n",
        "\n",
        "Data can be downloaded from the resources below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X40c-DTKk9hF",
        "outputId": "4796a836-f940-4191-bd9d-c7871597804e"
      },
      "source": [
        "!pip install torch\n",
        "!pip install tqdm"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNZ0qRZ1JlIJ"
      },
      "source": [
        "###write the code for your assignment here\n",
        "german_text = ''' Das gegenwartssprachliche DWDS-Wörterbuch bietet Informationen zu Form und Bedeutung von über 230 000 Wörtern der deutschen Sprache. \n",
        "                  Das Wörterbuch basiert auf der digitalisierten Fassung des Wörterbuchs der deutschen Gegenwartssprache; es wird ergänzt durch Teile des Großen Wörterbuchs der deutschen Sprache. \n",
        "                  Diese Artikel werden durch die Projektgruppe des DWDS und ZDL laufend überarbeitet. Zusätzlich werden neue Wörter von der Projektgruppe korpusbasiert ermittelt und lexikographisch beschrieben. \n",
        "                  Die Ergebnisse dieser Arbeiten werden wöchentlich auf der Startseite des DWDS unter der Rubrik „Neueste Artikel“ publiziert. '''\n",
        "\n",
        "russian_text = '''Когда моя дочь прочитала эту историю, она сказала: “Ты, папа, написал хороший рассказ. Но твой студент Игорь очень несовременно объясняется Вере в любви. \n",
        "                  Ты должен изменить это место. Современный молодой человек не может сказать только три слова, когда он объясняется в любви. \n",
        "                  Это очень просто: “Я тебя люблю!” Никто уже сейчас так не говорит. Ведь твои герои – будущие архитекторы. \n",
        "                  Я думаю, что Игорь должен сказать любимой девушке: “Вера, когда мы окончим институт, мы вместе поедем далеко-далеко строить новый город и нашу новую жизнь”. Это будет красиво, – сказала Оля.'''"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-24ZLRfYkyN5"
      },
      "source": [
        "###Russian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0muNSVVlnMK"
      },
      "source": [
        "normalizer from https://github.com/snakers4/russian_stt_text_normalization/blob/master/normalizer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmIOxyg1ll3H"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "from string import printable, punctuation\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self,\n",
        "                 device='cpu',\n",
        "                 jit_model='jit_s2s.pt'):\n",
        "        super(Normalizer, self).__init__()\n",
        "\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        self.init_vocabs()\n",
        "\n",
        "        self.model = torch.jit.load(jit_model, map_location=device)\n",
        "        self.model.eval()\n",
        "        self.max_len = 150\n",
        "\n",
        "    def init_vocabs(self):\n",
        "        # Initializes source and target vocabularies\n",
        "\n",
        "        # vocabs\n",
        "        rus_letters = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
        "        spec_symbols = '¼³№¾⅞½⅔⅓⅛⅜²'\n",
        "        # numbers + eng + punctuation + space + rus\n",
        "        self.src_vocab = {token: i + 5 for i, token in enumerate(printable[:-5] + rus_letters + '«»—' + spec_symbols)}\n",
        "        # punctuation + space + rus\n",
        "        self.tgt_vocab = {token: i + 5 for i, token in enumerate(punctuation + rus_letters + ' ' + '«»—')}\n",
        "\n",
        "        unk = '#UNK#'\n",
        "        pad = '#PAD#'\n",
        "        sos = '#SOS#'\n",
        "        eos = '#EOS#'\n",
        "        tfo = '#TFO#'\n",
        "        for i, token in enumerate([unk, pad, sos, eos, tfo]):\n",
        "            self.src_vocab[token] = i\n",
        "            self.tgt_vocab[token] = i\n",
        "\n",
        "        for i, token_name in enumerate(['unk', 'pad', 'sos', 'eos', 'tfo']):\n",
        "            setattr(self, '{}_index'.format(token_name), i)\n",
        "\n",
        "        inv_src_vocab = {v: k for k, v in self.src_vocab.items()}\n",
        "        self.src2tgt = {src_i: self.tgt_vocab.get(src_symb, -1) for src_i, src_symb in inv_src_vocab.items()}\n",
        "\n",
        "    def keep_unknown(self, string):\n",
        "        reg = re.compile(r'[^{}]+'.format(''.join(self.src_vocab.keys())))\n",
        "        unk_list = re.findall(reg, string)\n",
        "\n",
        "        unk_ids = [range(m.start() + 1, m.end()) for m in re.finditer(reg, string) if m.end() - m.start() > 1]\n",
        "        flat_unk_ids = [i for sublist in unk_ids for i in sublist]\n",
        "\n",
        "        upd_string = ''.join([s for i, s in enumerate(string) if i not in flat_unk_ids])\n",
        "        return upd_string, unk_list\n",
        "\n",
        "    def _norm_string(self, string):\n",
        "        # Normalizes chunk\n",
        "\n",
        "        if len(string) == 0:\n",
        "            return string\n",
        "        string, unk_list = self.keep_unknown(string)\n",
        "\n",
        "        token_src_list = [self.src_vocab.get(s, self.unk_index) for s in list(string)]\n",
        "        src = token_src_list + [self.eos_index] + [self.pad_index]\n",
        "\n",
        "        src2tgt = [self.src2tgt[s] for s in src]\n",
        "        src2tgt = torch.LongTensor(src2tgt).to(self.device)\n",
        "\n",
        "        src = torch.LongTensor(src).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.model(src, src2tgt)\n",
        "        pred_words = self.decode_words(out, unk_list)\n",
        "        if len(pred_words) > 199:\n",
        "            warnings.warn(\"Sentence {} is too long\".format(string), Warning)\n",
        "        return pred_words\n",
        "\n",
        "    def norm_text(self, text):\n",
        "        # Normalizes text\n",
        "\n",
        "        # Splits sentences to small chunks with weighted length <= max_len:\n",
        "        # * weighted length - estimated length of normalized sentence\n",
        "        #\n",
        "        # 1. Full text is splitted by \"ending\" symbols (\\n\\t?!.) to sentences;\n",
        "        # 2. Long sentences additionally splitted to chunks: by spaces or just dividing too long words\n",
        "\n",
        "        splitters = '\\n\\t?!'\n",
        "        parts = [p for p in re.split(r'({})'.format('|\\\\'.join(splitters)), text) if p != '']\n",
        "        norm_parts = []\n",
        "        for part in tqdm(parts):\n",
        "            if part in splitters:\n",
        "                norm_parts.append(part)\n",
        "            else:\n",
        "                weighted_string = [7 if symb.isdigit() else 1 for symb in part]\n",
        "                if sum(weighted_string) <= self.max_len:\n",
        "                    norm_parts.append(self._norm_string(part))\n",
        "                else:\n",
        "                    spaces = [m.start() for m in re.finditer(' ', part)]\n",
        "                    start_point = 0\n",
        "                    end_point = 0\n",
        "                    curr_point = 0\n",
        "\n",
        "                    while start_point < len(part):\n",
        "                        if curr_point in spaces:\n",
        "                            if sum(weighted_string[start_point:curr_point]) < self.max_len:\n",
        "                                end_point = curr_point + 1\n",
        "                            else:\n",
        "                                norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
        "                                start_point = end_point\n",
        "\n",
        "                        elif sum(weighted_string[end_point:curr_point]) >= self.max_len:\n",
        "                            if end_point > start_point:\n",
        "                                norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
        "                                start_point = end_point\n",
        "                            end_point = curr_point - 1\n",
        "                            norm_parts.append(self._norm_string(part[start_point:end_point]))\n",
        "                            start_point = end_point\n",
        "                        elif curr_point == len(part):\n",
        "                            norm_parts.append(self._norm_string(part[start_point:]))\n",
        "                            start_point = len(part)\n",
        "\n",
        "                        curr_point += 1\n",
        "        return ''.join(norm_parts)\n",
        "\n",
        "    def decode_words(self, pred, unk_list=None):\n",
        "        if unk_list is None:\n",
        "            unk_list = []\n",
        "        pred = pred.cpu().numpy()\n",
        "        pred_words = \"\".join(self.lookup_words(x=pred,\n",
        "                                               vocab={i: w for w, i in self.tgt_vocab.items()},\n",
        "                                               unk_list=unk_list))\n",
        "        return pred_words\n",
        "\n",
        "    def lookup_words(self, x, vocab, unk_list=None):\n",
        "        if unk_list is None:\n",
        "            unk_list = []\n",
        "        result = []\n",
        "        for i in x:\n",
        "            if i == self.unk_index:\n",
        "                if len(unk_list) > 0:\n",
        "                    result.append(unk_list.pop(0))\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                result.append(vocab[i])\n",
        "        return [str(t) for t in result]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuZ2nR7aknre",
        "outputId": "80790517-4413-4d9d-98c3-d1b0bd88002d"
      },
      "source": [
        "norm = Normalizer()\n",
        "russian_text = norm.norm_text(russian_text)\n",
        "print(russian_text)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 23.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Когда моя дочь прочитала эту историю, она сказала: “Ты, папа, написал хороший рассказ. Но твой студент Игорь очень несовременно объясняется Вере в любви. \n",
            "                  Ты должен изменить это место. Современный молодой человек не может сказать только три слова, когда он объясняется в любви. \n",
            "                  Это очень просто: “Я тебя люблю!” Никто уже сейчас так не говорит. Ведь твои герои – будущие архитекторы. \n",
            "                  Я думаю, что Игорь должен сказать любимой девушке: “Вера, когда мы окончим институт, мы вместе поедем далеко-далеко строить новыи ̆ аи город и нашу новую жизнь”. Это будет красиво, – сказала Оля.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzpkwP0hmZMS"
      },
      "source": [
        "!pip install pymorphy2[fast]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS8SAVwpmyeL"
      },
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0aG9raRnWAD",
        "outputId": "71992c27-ad3a-4ef8-e152-1636e9a333f9"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp_ru = spacy.load('ru_core_news_sm')\n",
        "stop_words_spacy_ru = nlp_ru.Defaults.stop_words\n",
        "print(len(stop_words_spacy_ru))\n",
        "print(stop_words_spacy_ru)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264\n",
            "{'неё', 'сам', 'своими', 'наши', 'быть', 'всё', 'его', 'могут', 'сама', 'мной', 'тобою', 'мне', 'само', 'на', 'к', 'от', 'по', 'теми', 'самих', 'вами', 'моими', 'нет', 'нашею', 'тот', 'была', 'мною', 'моё', 'им', 'нашим', 'таким', 'который', 'моём', 'нашей', 'своему', 'себе', 'будет', 'нами', 'весь', 'томах', 'это', 'которым', 'той', 'не', 'наших', 'все', 'одно', 'такая', 'собою', 'себя', 'этими', 'которую', 'об', 'свои', 'него', 'есть', 'нашему', 'у', 'всего', 'так', 'то', 'вы', 'нее', 'ими', 'эти', 'тебе', 'самом', 'самому', 'моем', 'наша', 'саму', 'такое', 'нему', 'нашу', 'одного', 'уже', 'нею', 'самого', 'своим', 'можем', 'такую', 'можете', 'был', 'до', 'только', 'всю', 'ему', 'мой', 'тобой', 'кем', 'ещё', 'тех', 'мою', 'а', 'нам', 'если', 'ей', 'самим', 'такими', 'можешь', 'этою', 'таком', 'оно', 'чём', 'вам', 'ним', 'одних', 'та', 'вся', 'эту', 'ней', 'во', 'для', 'всей', 'могли', 'могите', 'будут', 'её', 'имъ', 'одна', 'свой', 'чем', 'этом', 'ко', 'комья', 'всему', 'было', 'чему', 'одном', 'моим', 'ею', 'мы', 'которою', 'одной', 'всём', 'мочь', 'свою', 'за', 'этому', 'были', 'ними', 'одною', 'едят', 'будем', 'одним', 'своих', 'том', 'тем', 'или', 'своей', 'которого', 'своём', 'нем', 'всеми', 'того', 'в', 'мои', 'будучи', 'меня', 'моих', 'вас', 'он', 'она', 'нашими', 'которыми', 'один', 'одному', 'же', 'самими', 'я', 'но', 'ел', 'такие', 'кого', 'кому', 'которому', 'одними', 'да', 'свое', 'такой', 'всем', 'моги', 'ее', 'может', 'нём', 'всея', 'тебя', 'ту', 'ела', 'которое', 'оне', 'ком', 'чего', 'кто', 'моего', 'сами', 'емъ', 'ем', 'такому', 'с', 'и', 'при', 'которых', 'когда', 'из', 'наше', 'что', 'всех', 'которые', 'нас', 'всею', 'будьте', 'нашего', 'едим', 'одну', 'одни', 'таких', 'будь', 'моя', 'которой', 'такого', 'буду', 'мое', 'моей', 'мог', 'они', 'этим', 'могу', 'чтобы', 'собой', 'еще', 'будете', 'своем', 'ешь', 'этих', 'вот', 'бы', 'тою', 'них', 'ты', 'этой', 'своё', 'как', 'моею', 'о', 'эта', 'своего', 'этого', 'своя', 'их', 'тому', 'моему', 'своею', 'такою', 'ест', 'будешь', 'могло', 'наш', 'могла', 'нашем', 'этот', 'которая', 'наса', 'котором', 'те'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSLPWzLwnaLQ",
        "outputId": "7a81113d-1cbd-477e-b5f1-d076485b2451"
      },
      "source": [
        "tokenized_text_spacy_ru = nlp_ru(russian_text)\n",
        "tokenized_text_without_stopwords_ru = [i for i in tokenized_text_spacy_ru if not i in stop_words_spacy_ru]\n",
        "print(tokenized_text_without_stopwords_ru)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Когда, моя, дочь, прочитала, эту, историю, ,, она, сказала, :, “, Ты, ,, папа, ,, написал, хороший, рассказ, ., Но, твой, студент, Игорь, очень, несовременно, объясняется, Вере, в, любви, ., \n",
            "                  , Ты, должен, изменить, это, место, ., Современный, молодой, человек, не, может, сказать, только, три, слова, ,, когда, он, объясняется, в, любви, ., \n",
            "                  , Это, очень, просто, :, “, Я, тебя, люблю, !, ”, Никто, уже, сейчас, так, не, говорит, ., Ведь, твои, герои, –, будущие, архитекторы, ., \n",
            "                  , Я, думаю, ,, что, Игорь, должен, сказать, любимой, девушке, :, “, Вера, ,, когда, мы, окончим, институт, ,, мы, вместе, поедем, далеко, -, далеко, строить, новыи, ̆, аи, город, и, нашу, новую, жизнь, ”, ., Это, будет, красиво, ,, –, сказала, Оля, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Resources:\n",
        "\n",
        "* [An Crúbadán - Corpus Building for Minority Languages - 18721 languages](http://crubadan.org/)\n",
        "* [Leipzig Corpora Collection / Deutscher Wortschatz 291 languages](https://wortschatz.uni-leipzig.de/en/download)\n",
        "* [Europarl](https://www.statmt.org/europarl/)\n",
        "\n",
        "Tools:\n",
        "* [Chinese text segmentation](https://github.com/fxsjy/jieba)\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [A Survey of Approaches to Diacritic Restoration](https://www.researchgate.net/profile/Franklin-Asahiah/publication/328419851_A_Survey_of_Approaches_to_Diacritic_Restoration/links/5bcd8b67458515f7d9d02f3d/A-Survey-of-Approaches-to-Diacritic-Restoration.pdf)\n",
        "* [Preprocessing Arabic text on social media](https://www.sciencedirect.com/science/article/pii/S2405844021002966)\n",
        "* [Semantic-Based Segmentation of Arabic Texts](https://scialert.net/fulltext/?doi=itj.2008.1009.1015)\n",
        "* [The case of Croatian](https://medium.com/krakensystems-blog/text-processing-problems-with-non-english-languages-82822d0945dd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMxu5w7goaYC"
      },
      "source": [
        "###German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edpItNAloc2I",
        "outputId": "6a172005-4310-4cec-8b76-fa2325389e9e"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "stop_words = set(stopwords.words(\"german\"))\n",
        "\n",
        "\n",
        "def clean_text(text, for_embedding=False):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX-PcVhQqGtA",
        "outputId": "04ceb8e5-aa70-4f2b-ac26-14349a3d11c2"
      },
      "source": [
        "print(clean_text(german_text))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gegenwartssprach dwds worterbuch bietet information form bedeut wort deutsch sprach worterbuch basiert digitalisiert fassung worterbuch deutsch gegenwartssprach erganzt teil gross worterbuch deutsch sprach artikel projektgrupp dwds zdl laufend uberarbeitet zusatz neu wort projektgrupp korpusbasiert ermittelt lexikograph beschrieb ergebnis arbeit wochent startseit dwds rubrik neu artikel publiziert\n"
          ]
        }
      ]
    }
  ]
}